{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = (random.randint(0, self.size-1), random.randint(0, self.size-1))\n",
    "        self.passenger_position = (random.randint(0, self.size-1), random.randint(0, self.size-1))\n",
    "        self.destination_position = (random.randint(0, self.size-1), random.randint(0, self.size-1))\n",
    "        self.passenger_in_taxi = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.agent_position, self.passenger_position, self.destination_position, self.passenger_in_taxi\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.agent_position\n",
    "\n",
    "        if action == 'N':\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 'S':\n",
    "            x = min(self.size - 1, x + 1)\n",
    "        elif action == 'E':\n",
    "            y = min(self.size - 1, y + 1)\n",
    "        elif action == 'W':\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 'P':\n",
    "            if self.agent_position == self.passenger_position:\n",
    "                self.passenger_in_taxi = True\n",
    "                self.passenger_position = None\n",
    "        elif action == 'D':\n",
    "            if self.agent_position == self.destination_position and self.passenger_in_taxi:\n",
    "                self.passenger_in_taxi = False\n",
    "                self.destination_position = None\n",
    "\n",
    "        self.agent_position = (x, y)\n",
    "\n",
    "        reward = -1  # Default reward is -1 for each step\n",
    "\n",
    "        if action == 'P' and self.passenger_in_taxi:\n",
    "            reward += 10  # Picked up the passenger\n",
    "        elif action == 'D' and self.destination_position is None:\n",
    "            reward += 20  # Dropped off the passenger\n",
    "        elif action in ['P', 'D']:\n",
    "            reward -= 10  # Unsuccessful pick-up or drop-off\n",
    "\n",
    "        return self.agent_position, reward, self.destination_position is None\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actions = ['N', 'S', 'E', 'W', 'P', 'D']\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return random.choice(self.actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hier ist eine grundlegende Implementierung eines Q-Learning-Agenten:\n",
    "\n",
    "- In dieser Implementierung verwendet der Agent die Epsilon-Greedy-Strategie, um die Aktionen auszuwählen. Das bedeutet, dass der Agent mit einer Wahrscheinlichkeit von Epsilon (exploration_rate) eine zufällige Aktion wählt (dies ist der Explorationsteil), und mit einer Wahrscheinlichkeit von 1 - Epsilon die Aktion mit der höchsten erwarteten zukünftigen Belohnung wählt (dies ist der Exploitationsteil).\n",
    "\n",
    "- Die update_q_value-Methode implementiert die Q-Learning-Update-Regel. Diese Regel sagt, dass die erwartete zukünftige Belohnung für eine Aktion in einem Zustand gleich der aktuellen Belohnung plus der maximalen erwarteten zukünftigen Belohnung im nächsten Zustand ist (abgezinst durch den Discount-Faktor), minus der aktuellen erwarteten zukünftigen Belohnung.\n",
    "\n",
    "- Der Agent speichert die Q-Werte in einem Wörterbuch, wobei die Schlüssel Tupel aus Zustand und Aktion sind. Der Agent initialisiert die Q-Werte als 0.0, wenn sie noch nicht in der Tabelle sind.\n",
    "\n",
    "- Der nächste Schritt wäre, den Agenten in der Umgebung zu trainieren. Dies würde bedeuten, den Agenten mehrere Episoden lang in der Umgebung spielen zu lassen und die Q-Werte basierend auf den Belohnungen, die der Agent erhält, zu aktualisieren. Möchten Sie fortfahren?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, learning_rate=0.01, discount_factor=0.99, exploration_rate=0.1):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.df = discount_factor\n",
    "        self.er = exploration_rate\n",
    "        self.q_table = {}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.er:\n",
    "            # Explore: choose a random action\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest expected future reward\n",
    "            state = tuple(state)\n",
    "            q_values = [self.get_q_value(state, action) for action in self.actions]\n",
    "            return self.actions[np.argmax(q_values)]\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "\n",
    "        old_q_value = self.get_q_value(state, action)\n",
    "        max_next_q_value = max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "\n",
    "        # Q-learning update rule\n",
    "        new_q_value = old_q_value + self.lr * (reward + self.df * max_next_q_value - old_q_value)\n",
    "        self.q_table[(state, action)] = new_q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3, 4), (0, 1), (1, 1), False)\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "state = env.get_state()\n",
    "print(state)\n",
    "action = agent.choose_action(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_state (3, 4)\n",
      "reward -11\n",
      "done False\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, done = env.step(action)\n",
    "print('new_state', new_state)\n",
    "print('reward', reward)\n",
    "print('done', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In diesem Code trainieren wir den Agenten über 5000 Episoden. In jeder Episode wählt der Agent auf Basis des aktuellen Zustands eine Aktion, führt diese Aktion aus und erhält eine Belohnung und einen neuen Zustand. Dann aktualisiert der Agent die Q-Werte basierend auf der Belohnung und dem neuen Zustand. Der Agent wiederholt diesen Prozess, bis er den Passagier erfolgreich abgesetzt hat, was das Ende der Episode markiert.\n",
    "\n",
    "- Bitte beachten Sie, dass es mehrere Hyperparameter in diesem Code gibt, die Sie möglicherweise anpassen müssen, um gute Ergebnisse zu erzielen, darunter die Anzahl der Episoden, die Lernrate des Agenten, der Diskontierungsfaktor und die Erkundungsrate.\n",
    "\n",
    "- Nachdem der Agent trainiert wurde, können Sie ihn testen, indem Sie ihn in der Umgebung agieren lassen und sehen, wie gut er die Aufgabe erfüllt. Sie könnten auch die Q-Werte oder die erhaltene Belohnung im Laufe der Zeit darstellen, um zu sehen, wie der Agent lernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent and the environment\n",
    "agent = QLearningAgent(['N', 'S', 'E', 'W', 'P', 'D'])\n",
    "env = Environment()\n",
    "\n",
    "# Number of episodes to train the agent\n",
    "n_episodes = 10\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    print(episode)\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # The agent chooses an action\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # The agent performs the action and gets the reward and new state\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # The agent updates the Q-values\n",
    "        agent.update_q_value(state, action, reward, new_state)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            # The episode ends if the agent has delivered the passenger\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In diesem Code trainieren wir den Agenten über 5000 Episoden und verfolgen die Gesamtbelohnung, die der Agent in jeder Episode erhält. Am Ende visualisieren wir die Gesamtbelohnungen pro Episode, um zu sehen, wie sich die Leistung des Agenten im Laufe der Zeit verbessert.\n",
    "\n",
    "- Dieser Plot kann Ihnen helfen zu beurteilen, ob Ihr Agent lernt: Wenn der Agent richtig lernt, sollten Sie im Allgemeinen eine Verbesserung der Gesamtbelohnungen im Laufe der Zeit sehen, da der Agent lernt, den Passagier effektiver zu transportieren.\n",
    "\n",
    "- Bitte beachten Sie, dass die Leistung des Agenten von vielen Faktoren abhängt, einschließlich der Lernrate, des Diskontierungsfaktors, der Erkundungsrate und der Anzahl der Trainings-Episoden. Sie müssen möglicherweise mit diesen Parametern experimentieren, um gute Ergebnisse zu erzielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the agent and the environment\n",
    "agent = QLearningAgent(['N', 'S', 'E', 'W', 'P', 'D'])\n",
    "env = Environment()\n",
    "\n",
    "# Number of episodes to train the agent\n",
    "n_episodes = 5000\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # The agent chooses an action\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # The agent performs the action and gets the reward and new state\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # The agent updates the Q-values\n",
    "        agent.update_q_value(state, action, reward, new_state)\n",
    "\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            # The episode ends if the agent has delivered the passenger\n",
    "            break\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "# Plot the rewards\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.title('Total rewards per episode (training)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "n_test_episodes = 10\n",
    "test_rewards_per_episode = []\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        # The agent chooses an action\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # The agent performs the action and gets the reward and new state\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            # The episode ends if the agent has delivered the passenger\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total reward = {total_reward}, Steps = {steps}\")\n",
    "    test_rewards_per_episode.append(total_reward)\n",
    "\n",
    "# Print the average reward per episode\n",
    "print(f\"Average reward: {np.mean(test_rewards_per_episode)}\")\n",
    "\n",
    "# Plot the rewards\n",
    "plt.plot(test_rewards_per_episode)\n",
    "plt.title('Total rewards per episode (testing)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
